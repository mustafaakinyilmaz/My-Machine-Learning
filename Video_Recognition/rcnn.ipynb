{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import skimage as sk\n",
    "from natsort import natsorted\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "#torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pair_lists(train_test_path):\n",
    "    \n",
    "    class_names = [names for names in glob.glob(train_test_path+\"/*\")]\n",
    "    #print(class_names)\n",
    "    \n",
    "    a = []\n",
    "    for names in class_names:\n",
    "        folder = []\n",
    "        for folders in glob.glob(names+\"/*\"):\n",
    "            folder.append(folders)\n",
    "        a.append(folder)\n",
    "        \n",
    "\n",
    "    paired_folders = itertools.zip_longest(*a)\n",
    "    \n",
    "    return list(paired_folders)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sets(folder,label):\n",
    "    X_ = []\n",
    "    Y_ = []\n",
    "    frame_list = []\n",
    "    num_of_images = 0\n",
    "    for frame in glob.glob(folder+\"/*.jpg\"):\n",
    "        im_array = im_toarray(frame)\n",
    "        X_.append(im_array)\n",
    "        Y_.append(label)\n",
    "        num_of_images += 1\n",
    "    \n",
    "    return normalize(np.array(X_)),np.array(Y_),num_of_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_toarray(im):\n",
    "    array = plt.imread(im)\n",
    "    array = resize(array,(224,224,3),anti_aliasing=True)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(array, num_classes):\n",
    "    return np.squeeze(np.eye(num_classes)[array.reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    X_normalized = (X - mean)/std\n",
    "    return X_normalized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_tensor(array):\n",
    "    tensor = torch.from_numpy(array)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model():\n",
    "\n",
    "    model = models.resnet34(pretrained=True)\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    p = 0\n",
    "    for child in model.children():\n",
    "        if p > 4:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "        p += 1\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, flatten_dim, hidden_size, bias):\n",
    "        super(LSTMCell, self).__init__()\n",
    "    \n",
    "        self.flatten_dim = flatten_dim\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.bias        = bias\n",
    "        \n",
    "        self.i2h = nn.Linear(flatten_dim, 4*hidden_size, bias=bias)\n",
    "        self.h2h = nn.Linear(hidden_size, 4*hidden_size, bias=bias)\n",
    "        \n",
    " \n",
    "        \n",
    "    def forward(self, x, cur_state):\n",
    "        \n",
    "        if cur_state is None:\n",
    "            cur_state = (Variable(torch.zeros(1, self.hidden_size)).to(device).float(),\n",
    "                Variable(torch.zeros(1, self.hidden_size)).to(device).float())\n",
    "        \n",
    "        x = x.view(1,-1)\n",
    "        c_cur, h_cur = cur_state\n",
    "        preact = self.i2h(x) + self.h2h(h_cur)\n",
    "        #print(preact.size())\n",
    "        ingate, forgetgate, cellgate, outgate = preact.chunk(4, 1)\n",
    "        \n",
    "        \n",
    "        ingate = F.sigmoid(ingate)\n",
    "        forgetgate = F.sigmoid(forgetgate)\n",
    "        cellgate = F.tanh(cellgate)\n",
    "        outgate = F.sigmoid(outgate)\n",
    "        \n",
    "        c_next = (forgetgate * c_cur) + (ingate * cellgate)\n",
    "        h_next = outgate * F.tanh(c_next)\n",
    "        \n",
    "        next_state = (c_next,h_next)\n",
    "        \n",
    "        #softmax_out = F.softmax(self.linear(h_next.view(1,-1)),dim=1)\n",
    "        \n",
    "        return next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESLSTM(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(RESLSTM, self).__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential(*list(original_model.children())[:-1]).to(device).float()\n",
    "        #self.dropout = nn.Dropout(p=0.5)\n",
    "        self.lstm1 = LSTMCell(512,512,True).to(device).float()\n",
    "        self.lstm2 = LSTMCell(512,99,True).to(device).float()\n",
    "        #self.lstm3 = LSTMCell(128,99,True).to(device).float()\n",
    "\n",
    "        \n",
    "    def forward(self,x,state1,state2):\n",
    "        conv = self.cnn(x)\n",
    "        c1,h1 = self.lstm1(conv,state1)\n",
    "        c2,h2 = self.lstm2(h1,state2)\n",
    "        #c3,h3 = self.lstm3(h2,state3)\n",
    "        next_state1 = (c1,h1)\n",
    "        next_state2 = (c2,h2)\n",
    "        #next_state3 = (c3,h3)\n",
    "        softmax = F.softmax(h2,dim=1)    \n",
    "        return next_state1,next_state2,softmax\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_loss(out,real):\n",
    "\n",
    "    log_loss = torch.mean(-real*torch.log(out))\n",
    "    mean_loss = torch.mean((out-real)**2)\n",
    "    \n",
    "    loss = (mean_loss + log_loss)/2.0\n",
    "\n",
    "    return log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_X_Y(folder_class,target):\n",
    "    nb_classes = 99\n",
    "    X,Y,num_of_images = create_sets(folder_class,target)\n",
    "    X = X.reshape(-1,3,224,224)\n",
    "    Y = one_hot(Y,nb_classes)\n",
    "    X,Y = array_to_tensor(X).to(device).float(),array_to_tensor(Y).to(device).float()\n",
    "    return X,Y,num_of_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loader(pair_training,ind):\n",
    "    train_pairs = list(pair_training[ind])\n",
    "    notnone = len(train_pairs) - train_pairs.count(None)\n",
    "    print(str(notnone)+\"/\"+str(len(train_pairs)))\n",
    "    target = 0\n",
    "    \n",
    "    for folder_class in train_pairs:\n",
    "        \n",
    "        if folder_class is not None:\n",
    "            \n",
    "            X,Y,num_of_images = prepare_X_Y(folder_class,target)\n",
    "            yield (folder_class,X,Y,num_of_images,notnone)\n",
    "            \n",
    "        target += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    resnet34 = model().to(device).float()\n",
    "    res_lstm = RESLSTM(resnet34).to(device).float()\n",
    "    \n",
    "    pair_training = create_pair_lists(train_test_path= \"Frames/Train\")\n",
    "    pair_test =  create_pair_lists(train_test_path= \"Frames/Test\")\n",
    "    \n",
    "    nb_epoch = 25\n",
    "    lr = 3.e-1\n",
    "    optim = filter(lambda p: p.requires_grad,res_lstm.parameters())  \n",
    "    optimizer = torch.optim.RMSprop(optim,lr)\n",
    "    \n",
    "    \n",
    "    for epoch in range(1,nb_epoch+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_correct = 0\n",
    "        epoch_notnone_videos = 0\n",
    "        epoch_images = 0\n",
    "        for ind in range(len(pair_training)):\n",
    "            train_batch = train_loader(pair_training,ind)\n",
    "            pair_loss = 0\n",
    "            pair_correct = 0\n",
    "            total_pair_images = 0\n",
    "            for folder_class,X,Y,num_of_images,notnone in train_batch:\n",
    "                #print(folder_class)\n",
    "                folder_class_loss,folder_correct = train(X,Y,res_lstm,optimizer)\n",
    "                pair_loss += folder_class_loss\n",
    "                pair_correct += folder_correct\n",
    "                total_pair_images += num_of_images\n",
    "            \n",
    "            print(\"pair: \"+str(ind))\n",
    "            print(\"there are \"+str(total_pair_images)+\" images in the pair\")\n",
    "            print(\"pair loss: \"+str(pair_loss.item()/total_pair_images))\n",
    "            print(\"pair accuracy: \"+str(pair_correct)+\"/\"+str(notnone))\n",
    "            print(\"********\")\n",
    "            epoch_loss += pair_loss\n",
    "            epoch_correct += pair_correct\n",
    "            epoch_images += total_pair_images\n",
    "            epoch_notnone_videos += notnone\n",
    "        \n",
    "        print(\"epoch: \"+str(epoch))\n",
    "        print(\"there are \"+str(epoch_images)+\" images in the data\")\n",
    "        print(\"epoch loss: \"+str(epoch_loss.item()/epoch_images))\n",
    "        print(\"epoch accuracy: \"+str(epoch_correct)+\"/\"+str(epoch_notnone_videos))\n",
    "        print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X,Y,model,optimizer):\n",
    "    folder_class_loss = 0\n",
    "    \n",
    "    state1 = None\n",
    "    state2 = None\n",
    "    softmax_average = 0\n",
    "    \n",
    "    for f in range(0,X.size()[0]):\n",
    "        inp = X[f:f+1]\n",
    "        real = Y[f:f+1]\n",
    "                        \n",
    "        state1,state2,softmax_out = model(inp,state1,state2)\n",
    "        loss = calculate_loss(softmax_out,real)\n",
    "        folder_class_loss += loss\n",
    "        softmax_average += softmax_out\n",
    "        \n",
    "    folder_class_average_loss = folder_class_loss/X.size(0)\n",
    "    softmax_average = softmax_average/X.size(0)\n",
    "    \n",
    "    correct = 0\n",
    "    if torch.argmax(softmax_average,dim=1).item() == torch.argmax(real,dim=1).item():\n",
    "        correct = 1\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "                    \n",
    "    folder_class_average_loss.backward()\n",
    "                    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return folder_class_loss,correct\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"folder1 = \"Frames/Train/ApplyLipstick/1\"\n",
    "X1,Y1 = create_sets(folder1,78)\n",
    "X1 = X1.reshape(-1,3,224,224)\n",
    "Y1 = one_hot(Y1,99)\n",
    "X1,Y1 = array_to_tensor(X1).to(device).float(),array_to_tensor(Y1).to(device).float()\n",
    "\n",
    "folder2 = \"Frames/Train/Bowling/1\"\n",
    "X2,Y2 = create_sets(folder2,55)\n",
    "X2 = X2.reshape(-1,3,224,224)\n",
    "Y2 = one_hot(Y2,99)\n",
    "X2,Y2 = array_to_tensor(X2).to(device).float(),array_to_tensor(Y2).to(device).float()\n",
    "\n",
    "xlist = [X1,X2]\n",
    "ylist = [Y1,Y2]\n",
    "\n",
    "optim_cnn = filter(lambda p: p.requires_grad,res_lstm.cnn.parameters())    \n",
    "optim_lstm1 = filter(lambda p: p.requires_grad,res_lstm.lstm1.parameters()) \n",
    "optim_lstm2 = filter(lambda p: p.requires_grad,res_lstm.lstm2.parameters()) \n",
    "#optim_lstm3 = filter(lambda p: p.requires_grad,res_lstm.lstm3.parameters()) \n",
    "\n",
    "#a = torch.rand(6,3,224,224).to(device).float()\n",
    "#b = array_to_tensor(one_hot(np.array([3,3,3,3,3,3]),99)).to(device).float()\n",
    "\n",
    "optimizer_cnn = torch.optim.Adam(optim_cnn,lr=5.e-2)\n",
    "optimizer_lstm1 = torch.optim.SGD(optim_lstm1,lr=3.e-1)\n",
    "optimizer_lstm2 = torch.optim.SGD(optim_lstm2,lr=3.e-1)\n",
    "#optimizer_lstm3 = torch.optim.Adam(optim_lstm3,lr=3.e-2)\n",
    "X1.is_cuda\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"model_parameters = filter(lambda p: p.requires_grad, res_lstm.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])  \n",
    "print(params)\n",
    "\n",
    "loss_list = []\n",
    "epoch_list = []\n",
    "for epoch in range(1,501):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for l in range(2):\n",
    "        X = xlist[l]\n",
    "        Y = ylist[l]\n",
    "        folder_loss = 0\n",
    "        \n",
    "        state1 = None\n",
    "        state2 = None\n",
    "        for f in range(0,X.size()[0]):\n",
    "            inp = X[f:f+1]\n",
    "            real = Y[f:f+1]\n",
    "            state1,state2,softmax_out = res_lstm(inp,state1,state2)\n",
    "            loss,correct = evaluate(softmax_out,real)\n",
    "            folder_loss += loss\n",
    "        \n",
    "        folder_loss = folder_loss/X.size()[0]\n",
    "    \n",
    "\n",
    "        optimizer_lstm2.zero_grad()\n",
    "        optimizer_lstm1.zero_grad()\n",
    "        optimizer_cnn.zero_grad()\n",
    "\n",
    "        folder_loss.backward()\n",
    "\n",
    "\n",
    "        optimizer_lstm2.step()\n",
    "        optimizer_lstm1.step()\n",
    "        optimizer_cnn.step()\n",
    "        \n",
    "        index = torch.argmax(softmax_out).item()\n",
    "\n",
    "        \n",
    "    epoch_loss += folder_loss\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"epoch: \"+str(epoch))\n",
    "        print(\"loss: \"+str(epoch_loss))\n",
    "        index = torch.argmax(softmax_out).item()\n",
    "        print(index)\n",
    "        print(softmax_out[:,index].item()*100)\n",
    "        print(\"*************\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(k=maxk, largest=True, sorted=True)\n",
    "    pred = pred.t()\n",
    "    pred[2] = 78\n",
    "    print(pred)\n",
    "    print(target)\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    print(correct)\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        print(k)\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Y1[0:1,:]\n",
    "x = X1[0:1,:,:,:]\n",
    "s,s2,out = res_lstm(x,None,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ = torch.argmax(out,dim=1)\n",
    "real = torch.argmax(y,dim=1)\n",
    "print(out_,real)\n",
    "res = accuracy(out,real,(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [1,2,None,3,None]\n",
    "b.count(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
