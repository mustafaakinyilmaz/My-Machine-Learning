{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucf_data_path = \"UCF-101\"\n",
    "train_video_path = ucf_data_path+\"/Train\"\n",
    "test_video_path = ucf_data_path+\"/Test\"\n",
    "\n",
    "train_archery = train_video_path + \"/Archery\"\n",
    "train_makeup = train_video_path + \"/Makeup\"\n",
    "\n",
    "test_archery = test_video_path + \"/Archery\"\n",
    "test_makeup = test_video_path + \"/Makeup\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_toarray(im):\n",
    "    array = plt.imread(im)\n",
    "    array = resize(array,(224,224))\n",
    "    return array\n",
    "    \n",
    "\n",
    "def create_dataset():\n",
    "    Y_archery = []\n",
    "    archery_whole_frames = []\n",
    "    for folders in glob.glob(train_archery+\"/*\"):\n",
    "        one_video_frames = []\n",
    "        Y_archery.append(1)\n",
    "        for jpgs in glob.glob(folders+\"/*.jpg\"):\n",
    "            arch_im = im_toarray(jpgs)\n",
    "            one_video_frames.append(arch_im)\n",
    "        \n",
    "        archery_whole_frames.append(one_video_frames)\n",
    "    \n",
    "    Y_makeup = []\n",
    "    makeup_whole_frames = []\n",
    "    for folders in glob.glob(train_makeup+\"/*\"):\n",
    "        one_video_frames = []\n",
    "        Y_makeup.append(0)\n",
    "        for jpgs in glob.glob(folders+\"/*.jpg\"):\n",
    "            makeup_im = im_toarray(jpgs)\n",
    "            one_video_frames.append(makeup_im)\n",
    "        \n",
    "        makeup_whole_frames.append(one_video_frames)\n",
    "        \n",
    "    X_train = np.concatenate((np.array(archery_whole_frames),np.array(makeup_whole_frames)),axis = 0)\n",
    "    Y_train = np.concatenate((np.array(Y_archery),np.array(Y_makeup)))\n",
    "    \n",
    "    m = X_train.shape[0]\n",
    "    nf = X_train.shape[1]\n",
    "    nw = X_train.shape[2]\n",
    "    nh = X_train.shape[3]\n",
    "    nc = X_train.shape[4]\n",
    "    \n",
    "    return X_train.reshape(m,nf,nc,nw,nh),Y_train.reshape(16,1)\n",
    "    \n",
    "X_train,Y_train = create_dataset()\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "model_cnn = models.vgg16(pretrained=True)\n",
    "for param in model_cnn.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.feature_extractor = original_model.features\n",
    "        self.rnn =  nn.GRUCell(512*7*7,1)\n",
    "        self.hidden = torch.randn(1,1)\n",
    "        self.linear = nn.Linear(9,1,bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        \n",
    "        \n",
    "        o = torch.Tensor()\n",
    "        for i in range(9):\n",
    "            inp = x[i,:,:,:].view(-1,512*7*7)\n",
    "            self.hidden = self.rnn(inp,self.hidden)\n",
    "            o = torch.cat((o,self.hidden))\n",
    "            \n",
    "        x = self.linear(o.view(1,9))\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "        \n",
    "cnn_lstm = CNN_LSTM(model_cnn).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in cnn_lstm.rnn.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in cnn_lstm.feature_extractor[23:30].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in cnn_lstm.linear.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"using gpu\")\n",
    "    out = cnn_lstm(X[0,:,:,:,:])\n",
    "else:\n",
    "    out = cnn_lstm(X[0,:,:,:,:])\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(out,Y):\n",
    "    loss = torch.mean((out-Y)**2)\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,X_train,Y_train,nb_epoch,lr):\n",
    "    \n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad,model.parameters()),lr=lr)\n",
    "    \n",
    "    X_train, Y_train = torch.from_numpy(X_train).to(device).float(), torch.from_numpy(Y_train).to(device).float()\n",
    "    \n",
    "    for epoch in range(1,nb_epoch+1):\n",
    "        epoch_loss = 0.0\n",
    "        for video_no in range(16):\n",
    "            X_batch, Y_batch = X_train[video_no,:,:,:,:], Y_train[video_no,0]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out = model(X_batch)\n",
    "            loss = compute_loss(out,Y_batch)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        print(\"epoch: \"+str(epoch))\n",
    "        print(\"loss: \"+str(epoch_loss/16.0))\n",
    "        print(out)\n",
    "        print(Y_batch)\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "loss: 0.554722058965126\n",
      "tensor([[ 0.9419]])\n",
      "tensor(0.)\n",
      "epoch: 2\n",
      "loss: 0.2724910478864331\n",
      "tensor([[ 0.3023]])\n",
      "tensor(0.)\n",
      "epoch: 3\n",
      "loss: 0.33887993649113923\n",
      "tensor([[ 0.1575]])\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "train(cnn_lstm,X_train,Y_train,3,5.e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
